{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\ANN_Banking_Prediction\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\ANN_Banking_Prediction\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\ANN_Banking_Prediction\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Best: 0.865625 using {'neurons': 16, 'layers': 2, 'epochs': 75, 'batch_size': 64}\n",
      "63/63 [==============================] - 0s 902us/step - loss: 0.3426 - accuracy: 0.8560\n",
      "Test accuracy: 0.8560000061988831\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"Churn_Modelling.csv\")\n",
    "\n",
    "# Encode 'Gender' column\n",
    "l = LabelEncoder()\n",
    "data['Gender'] = l.fit_transform(data['Gender'])\n",
    "\n",
    "# ONE HOT ENCODING\n",
    "o = OneHotEncoder(handle_unknown='ignore')\n",
    "geo = o.fit_transform(data[['Geography']]).toarray()\n",
    "geo_df = pd.DataFrame(geo, columns=o.get_feature_names_out(['Geography']))\n",
    "\n",
    "data = pd.concat([data.drop(['Geography'], axis=1), geo_df], axis=1)\n",
    "\n",
    "# Separate features and target\n",
    "x = data.drop(['Exited', 'RowNumber', 'CustomerId', 'Surname'], axis=1)\n",
    "y = data['Exited']\n",
    "\n",
    "# Split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# CREATING A MODEL FROM SCRATCH\n",
    "def create_model(neurons=32, layers=1):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "    \n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'neurons': [16, 32, 64],\n",
    "    'layers': [1, 2, 3],\n",
    "    'epochs': [50, 75],\n",
    "    'batch_size': [32, 64]\n",
    "}\n",
    "\n",
    "# Perform manual grid search\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for neurons in param_grid['neurons']:\n",
    "    for layers in param_grid['layers']:\n",
    "        for epochs in param_grid['epochs']:\n",
    "            for batch_size in param_grid['batch_size']:\n",
    "                model = create_model(neurons=neurons, layers=layers)\n",
    "                history = model.fit(\n",
    "                    x_train_scaled, y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=0\n",
    "                )\n",
    "                val_accuracy = max(history.history['val_accuracy'])\n",
    "                \n",
    "                if val_accuracy > best_score:\n",
    "                    best_score = val_accuracy\n",
    "                    best_params = {\n",
    "                        'neurons': neurons,\n",
    "                        'layers': layers,\n",
    "                        'epochs': epochs,\n",
    "                        'batch_size': batch_size\n",
    "                    }\n",
    "\n",
    "# Print best performance\n",
    "print(\"Best: %f using %s\" % (best_score, best_params))\n",
    "\n",
    "# Train the final model with best parameters\n",
    "final_model = create_model(neurons=best_params['neurons'], layers=best_params['layers'])\n",
    "final_model.fit(\n",
    "    x_train_scaled, y_train,\n",
    "    epochs=best_params['epochs'],\n",
    "    batch_size=best_params['batch_size'],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = final_model.evaluate(x_test_scaled, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
